{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Offensive and defensive ELO ratings to predict number of goals\n",
    "\n",
    "We keep track of two ratings for all teams offensive rating ($R_O$) and defensive rating ($R_D$).\n",
    "\n",
    "We can then predict the number of goals a team will score by taking the difference of their offensive rating and the opponent's defensive rating.\n",
    "\n",
    "The number of goals scored against them can be calculated by considering it from the opponent's perspective.\n",
    "\n",
    "$E[\\text{team}] = R_O[\\text{team}] - R_D[\\text{opponent}]$\n",
    "\n",
    "We can update a team's offensive rating by adding the difference between the actual number of goals and the expected goals multiplied by the learning rate.\n",
    "We can update a team's defensive rating by adding the difference between the expected goals scored against them and the actual number of goals scored against them multiplied by the learning rate.\n",
    "\n",
    "$R_O[\\text{team}] = R_O[\\text{team}] + k(G[\\text{team}] - E[\\text{team}])$\n",
    "\n",
    "$R_D[\\text{team}] = R_D[\\text{team}] + k(E[\\text{opponent}] - G[\\text{opponent}])$\n",
    "\n",
    "We start every team with a rating of 0. The order of the training data makes a difference to the model and so the training data should be in chronological order in order to account for teams changing over team.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = pd.read_csv(\"data/football-data.co.uk/updated-epl-training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_result(result):\n",
    "    if result == 'H':\n",
    "        return 1\n",
    "    elif result == 'A':\n",
    "        return 0\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalElo:\n",
    "    def __init__(self, initial_rating=0, learning_rate=0.2, draw_size=0.5):\n",
    "        self.offensive_ratings = defaultdict(lambda: initial_rating)\n",
    "        self.defensive_ratings = defaultdict(lambda: initial_rating)\n",
    "        self.match_count = defaultdict(lambda: 0)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.draw_size = draw_size\n",
    "\n",
    "    def predict(self, team, opponent):\n",
    "        ''' Predicts the number of goals team will score against opponent. '''\n",
    "        return self.offensive_ratings[team] - self.defensive_ratings[opponent]\n",
    "\n",
    "    def predict_result(self, team, opponent):\n",
    "        goals_scored = self.predict(team, opponent)\n",
    "        goals_conceded = self.predict(opponent, team)\n",
    "        goal_difference = goals_scored - goals_conceded\n",
    "        result = 1 if goal_difference > 0 else 0\n",
    "        if abs(goal_difference) < self.draw_size:\n",
    "            result = 0.5\n",
    "        return result\n",
    "\n",
    "    def update_match(self, home, away, home_actual_goals, away_actual_goals):\n",
    "        ''' Updates the offensive and defensive ratings of both teams in a match. '''\n",
    "        home_expected_goals = self.predict(home, away)\n",
    "        away_expected_goals = self.predict(away, home)\n",
    "        self.offensive_ratings[home] += self.learning_rate * (home_actual_goals - home_expected_goals)\n",
    "        self.offensive_ratings[away] += self.learning_rate * (away_actual_goals - away_expected_goals)\n",
    "        self.defensive_ratings[home] += self.learning_rate * (away_expected_goals - away_actual_goals)\n",
    "        self.defensive_ratings[away] += self.learning_rate * (home_expected_goals - home_actual_goals)\n",
    "        self.match_count[home] += 1\n",
    "        self.match_count[away] += 1\n",
    "\n",
    "    def ratings_dataframe(self):\n",
    "        ''' Creates an easy to read dataframe of the ratings '''\n",
    "        df = pd.DataFrame(self.offensive_ratings.items(), columns=['Team', 'Offensive Rating'])\n",
    "        df['Defensive Rating'] = df['Team'].map(self.defensive_ratings)\n",
    "        df['Matches'] = df['Team'].map(self.match_count)\n",
    "        df = df.sort_values('Offensive Rating', ascending=False)\n",
    "        return df\n",
    "\n",
    "    def train(self, df):\n",
    "        ''' Takes a data frame of matches with columns HomeTeam, AwayTeam, FTHG, FTAG and updates teams ratings using the data in order. '''\n",
    "        for i, row in df.iterrows():\n",
    "            self.update_match(row['HomeTeam'], row['AwayTeam'], row['FTHG'], row['FTAG'])\n",
    "\n",
    "    def test(self, df):\n",
    "        ''' Takes a data frame of matches with columns HomeTeam, AwayTeam, FTHG, FTAG and uses the ratings to predict the number of goals scored by each side. It measures the average mean square error and average mean absolute error per match. '''\n",
    "        mse = 0\n",
    "        mae = 0\n",
    "        count = 0\n",
    "        for i, row in df.iterrows():\n",
    "            error_home = self.predict(row['HomeTeam'], row['AwayTeam']) - row['FTHG']\n",
    "            error_away = self.predict(row['AwayTeam'], row['HomeTeam']) - row['FTAG']\n",
    "            mse += error_home ** 2 + error_away ** 2\n",
    "            mae += abs(error_home) + abs(error_away)\n",
    "            count += 1\n",
    "        return mse / count, mae / count\n",
    "\n",
    "    def test_result(self, df):\n",
    "        ''' Takes a data frame of matches with columns HomeTeam, AwayTeam, FTR and predicts the outcome using the ratings. It measures the number of correct predictions as a percentage of the size of the data. '''\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for i, row in df.iterrows():\n",
    "            goal_difference = round(self.predict(row['HomeTeam'], row['AwayTeam']) - self.predict(row['AwayTeam'], row['HomeTeam']))\n",
    "            result = 0.5\n",
    "            if goal_difference > 0:\n",
    "                result = 1\n",
    "            if goal_difference < 0:\n",
    "                result = 0\n",
    "            if result == encode_result(row['FTR']):\n",
    "                correct += 1\n",
    "            count += 1\n",
    "        return correct / count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = sklearn.model_selection.train_test_split(match_data, test_size=0.05, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.97it/s]Accuracy:\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   Learning Rate  Mean Square Error / Match  Mean Absolute Error / Match  \\\n",
       "0         0.0001                   6.919693                     2.687661   \n",
       "1         0.0112                   3.482109                     2.038388   \n",
       "2         0.0223                   3.524080                     2.042918   \n",
       "3         0.0334                   3.547938                     2.041889   \n",
       "4         0.0445                   3.560787                     2.037325   \n",
       "5         0.0556                   3.570631                     2.033060   \n",
       "6         0.0667                   3.581140                     2.029605   \n",
       "7         0.0778                   3.593675                     2.027921   \n",
       "8         0.0889                   3.608569                     2.027247   \n",
       "9         0.1000                   3.625739                     2.027807   \n",
       "\n",
       "   Accuracy  \n",
       "0  0.240506  \n",
       "1  0.464135  \n",
       "2  0.464135  \n",
       "3  0.455696  \n",
       "4  0.464135  \n",
       "5  0.472574  \n",
       "6  0.459916  \n",
       "7  0.447257  \n",
       "8  0.438819  \n",
       "9  0.421941  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Learning Rate</th>\n      <th>Mean Square Error / Match</th>\n      <th>Mean Absolute Error / Match</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0001</td>\n      <td>6.919693</td>\n      <td>2.687661</td>\n      <td>0.240506</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0112</td>\n      <td>3.482109</td>\n      <td>2.038388</td>\n      <td>0.464135</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0223</td>\n      <td>3.524080</td>\n      <td>2.042918</td>\n      <td>0.464135</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0334</td>\n      <td>3.547938</td>\n      <td>2.041889</td>\n      <td>0.455696</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0445</td>\n      <td>3.560787</td>\n      <td>2.037325</td>\n      <td>0.464135</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.0556</td>\n      <td>3.570631</td>\n      <td>2.033060</td>\n      <td>0.472574</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.0667</td>\n      <td>3.581140</td>\n      <td>2.029605</td>\n      <td>0.459916</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.0778</td>\n      <td>3.593675</td>\n      <td>2.027921</td>\n      <td>0.447257</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.0889</td>\n      <td>3.608569</td>\n      <td>2.027247</td>\n      <td>0.438819</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.1000</td>\n      <td>3.625739</td>\n      <td>2.027807</td>\n      <td>0.421941</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "learning_rates = np.linspace(0.0001, 0.1, num=10)\n",
    "mse_list = []\n",
    "mae_list = []\n",
    "accuracy = []\n",
    "\n",
    "for learning_rate in tqdm(learning_rates):\n",
    "    goal_elo = GoalElo(learning_rate=learning_rate)\n",
    "    goal_elo.train(training)\n",
    "    mse, mae = goal_elo.test(test)\n",
    "    mse_list.append(mse)\n",
    "    mae_list.append(mae)\n",
    "    accuracy.append(goal_elo.test_result(test))\n",
    "print(\"Accuracy:\") \n",
    "pd.DataFrame(zip(learning_rates, mse_list, mae_list, accuracy), columns=['Learning Rate', 'Mean Square Error / Match', 'Mean Absolute Error / Match', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.149439628503313\n1.2314645420992298\n"
     ]
    }
   ],
   "source": [
    "goal_elo = ShotElo(learning_rate=0.1)\n",
    "goal_elo.train(training)\n",
    "# print(shot_elo.ratings_dataframe())\n",
    "print(goal_elo.predict('Man City', 'Chelsea'))\n",
    "print(goal_elo.predict('Chelsea', 'Man City'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}